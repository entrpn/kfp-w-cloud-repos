{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "9edd5dfe-ede5-46d4-8ef4-e010e343f9ef",
   "metadata": {},
   "source": [
    "## Intro\n",
    "This lab demonstrates how to launch a kubeflow pipeline using code located in source repositories. In this lab we will:\n",
    "- Create 2 cloud repos, one that holds the pipeline code and one that holds the training code. This will allow us to modularize the code and make it reusable across different scenarios.\n",
    "- Create a pipeline that reads from bq, creates a Vertex AI Dataset, generates data statistics using TensorFlow Data Validation library and runs a training job.\n",
    "- Send an email when the pipeline is completed.\n",
    "\n",
    "We'll be using the following GCP services and will need to enable them:\n",
    "- Vertex AI\n",
    "- BigQuery\n",
    "- Google Cloud Storage\n",
    "- Google Cloud Artifact Registry\n",
    "- Google Cloud Source Repositories\n",
    "- Google Cloud Artifact Repositories\n",
    "- Dataflow\n",
    "\n",
    "You'll also need the following permissions:\n",
    "- Vertex AI admin\n",
    "- Bigquery Admin\n",
    "- Google Cloud Storage object writer.\n",
    "- Dataflow Admin\n",
    "- Dataflow Worker\n",
    "- Artifact Registry Admin\n",
    "\n",
    "Network:\n",
    "- Firewall rule with TCP ports 12345-12346 (Dataflow)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6a6deb69-db07-4712-8b27-46b0a6a64e1a",
   "metadata": {},
   "source": [
    "## Setup\n",
    "Install libraries."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c3c094b2-a366-401a-b483-18f0b1edd360",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "!pip install --user google-cloud-aiplatform --upgrade --q\n",
    "!pip install --user kfp --upgrade --q\n",
    "!pip install --user google-cloud-pipeline-components --upgrade --q\n",
    "!pip install --user tensorflow_data_validation --upgrade -q"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4ebc6ce8-3e62-4a8f-bef9-84936909cd41",
   "metadata": {},
   "source": [
    "## Create the source repositories"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "815839f9-3e30-4cd7-8a81-0ae617018be4",
   "metadata": {},
   "outputs": [],
   "source": [
    "!gcloud source repos create my-kfp-library"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4b8a30b4-b38e-4d0e-a8fe-4fed52b80856",
   "metadata": {},
   "outputs": [],
   "source": [
    "!gcloud source repos clone my-kfp-library"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b11356ba-c2af-4acf-8f01-83ed97bdc912",
   "metadata": {},
   "outputs": [],
   "source": [
    "!gcloud source repos create my-training-code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "54762ced-18c8-4a7c-8ff2-446d06afd68f",
   "metadata": {},
   "outputs": [],
   "source": [
    "!gcloud source repos clone my-training-code"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f208f6f4-4363-4dfc-a450-868ea360e47d",
   "metadata": {},
   "source": [
    "## Create the training code and run the pipeline.\n",
    "We will build a container that contains the code to run a scikit-learn classifier and push it to Google Cloud Artifact Repository. The Kubeflow pipeline component will reference this container for the training step."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1789b931-af63-4a49-ae7f-ff09933f2d38",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%writefile my-training-code/Dockerfile\n",
    "FROM gcr.io/deeplearning-platform-release/sklearn-cpu.0-23\n",
    "WORKDIR /\n",
    "\n",
    "COPY trainer /trainer\n",
    "\n",
    "RUN pip install sklearn google-cloud-bigquery joblib pandas google-cloud-storage\n",
    "\n",
    "ENTRYPOINT [\"python\",\"-m\",\"trainer.train\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "70e7e97d-9d4b-4741-aaf2-b9805ca5b8ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "%mkdir my-training-code/trainer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6589e20c-bb46-44fa-acf6-ffd1812230b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%writefile my-training-code/trainer/train.py\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.metrics import roc_curve\n",
    "from sklearn.model_selection import train_test_split\n",
    "from google.cloud import bigquery\n",
    "from google.cloud import storage\n",
    "from joblib import dump\n",
    "\n",
    "import os\n",
    "import pandas as pd\n",
    "\n",
    "bqclient = bigquery.Client()\n",
    "storage_client = storage.Client()\n",
    "\n",
    "def download_table(bq_table_uri: str):\n",
    "    prefix = \"bq://\"\n",
    "    if bq_table_uri.startswith(prefix):\n",
    "        bq_table_uri = bq_table_uri[len(prefix):]\n",
    "    table = bigquery.TableReference.from_string(bq_table_uri)\n",
    "    rows = bqclient.list_rows(\n",
    "        table,\n",
    "    )\n",
    "    return rows.to_dataframe(create_bqstorage_client=False)\n",
    "\n",
    "# These environment variables are from Vertex AI managed datasets\n",
    "training_data_uri = os.environ[\"AIP_TRAINING_DATA_URI\"]\n",
    "test_data_uri = os.environ[\"AIP_TEST_DATA_URI\"]\n",
    "\n",
    "# Download data into Pandas df, split into train / test\n",
    "df = download_table(training_data_uri)\n",
    "test_df = download_table(test_data_uri)\n",
    "labels = df.pop(\"Class\").tolist()\n",
    "data = df.values.tolist()\n",
    "test_labels = test_df.pop(\"Class\").tolist()\n",
    "test_data = test_df.values.tolist()\n",
    "\n",
    "# Define and train the scikit model\n",
    "skmodel = DecisionTreeClassifier()\n",
    "skmodel.fit(data, labels)\n",
    "score = skmodel.score(test_data, test_labels)\n",
    "print('accuracy is:',score)\n",
    "\n",
    "dump(skmodel, 'model.joblib')\n",
    "\n",
    "# Upload the saved model file to GCS, the GCS_BUCKET variable will be passed \n",
    "# in the pipeline definition\n",
    "bucket = os.environ[\"GCS_BUCKET\"]\n",
    "model_directory = os.environ[\"AIP_MODEL_DIR\"]\n",
    "print(\"AIP_MODEL_DIR\",model_directory)\n",
    "storage_path = os.path.join(model_directory, \"model.joblib\")\n",
    "blob = storage.blob.Blob.from_string(storage_path, client=storage_client)\n",
    "blob.upload_from_filename(\"model.joblib\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e66f4d4b-add2-4a70-914f-65de93d4fefc",
   "metadata": {},
   "outputs": [],
   "source": [
    "PROJECT_ID = \"[your-project-id]\"  # @param {type:\"string\"}\n",
    "BUCKET_ID=\"[your-bucket-id]\"  # @param {type:\"string\"}\n",
    "IMAGE_URI=f\"gcr.io/{PROJECT_ID}/scikit:v1\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6601b515-7e40-48b9-a5ae-8fef78c99fae",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "!docker build ./my-training-code/ -t $IMAGE_URI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "08ea0b27-b2a7-4913-9591-f516c7c67479",
   "metadata": {},
   "outputs": [],
   "source": [
    "!docker push $IMAGE_URI"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d8b5af0c-5184-4d99-89d5-7877bc27f6e8",
   "metadata": {},
   "source": [
    "## Build the pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2b205ae0-3f36-4265-8612-a6cdcf722693",
   "metadata": {},
   "outputs": [],
   "source": [
    "%mkdir my-kfp-library/pipelines\n",
    "%mkdir my-kfp-library/components"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b8e202b1-3c61-45a1-ad79-63079428d4f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "!touch my-kfp-library/__init__.py\n",
    "!touch my-kfp-library/pipelines/__init__.py\n",
    "!touch my-kfp-library/components/__init__.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "44c0323b-591e-48b8-879e-11bb8d4980ba",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "!pip install --user tensorflow_data_validation\n",
    "!pip install --user pyparsing"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0c3346e6-1485-4951-8504-32993da04aac",
   "metadata": {},
   "source": [
    "### Create components\n",
    "Here we will create an empty component and a tensorflow data validator component that collects statistics about the input data."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "170089bd-aea2-44dc-ae53-5d4627e71356",
   "metadata": {},
   "source": [
    "**Emtpy component**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4ed3e077-4106-4ad5-a39e-003d6868cefd",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%writefile my-kfp-library/components/empty_component.py\n",
    "from kfp.v2.dsl import component\n",
    "@component()\n",
    "def empty_component():\n",
    "    print(\"this is a test component\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6352df17-9f55-476b-8ade-299218439187",
   "metadata": {},
   "source": [
    "**Generate Statistics component** will also have an option to run tfdv locally in the container."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "403fb3e1-1f93-4de7-b1a9-ecdb1821929a",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%writefile my-kfp-library/components/generate_statistics_component.py\n",
    "from kfp.v2.dsl import component, Output, Artifact\n",
    "\n",
    "# setting custom machine type settings https://cloud.google.com/vertex-ai/docs/pipelines/machine-types\n",
    "@component(base_image=\"tensorflow/tfx:1.8.0\", packages_to_install=[\"pandas\",\n",
    "                                \"google-cloud-bigquery\",\"google-cloud-storage\"])\n",
    "def generate_statistics(bq_source: str,\n",
    "                        bucket: str,\n",
    "                        job_id: str,\n",
    "                        project_id : str,\n",
    "                        statistics : Output[Artifact]):\n",
    "\n",
    "    import subprocess\n",
    "    from google.cloud import bigquery\n",
    "    import pandas as pd\n",
    "    import tensorflow_data_validation as tfdv\n",
    "    import sys\n",
    "\n",
    "    bqclient = bigquery.Client(project=project_id)\n",
    "\n",
    "    output_path = f'{bucket}/{job_id}/statistics/stats.pb'\n",
    "    \n",
    "    def remove_prefix(cloud_uri, prefix):\n",
    "        #prefix = \"bq://\"\n",
    "        #prefix = \"gs://\"\n",
    "        if cloud_uri.startswith(prefix):\n",
    "            cloud_uri = cloud_uri[len(prefix):]\n",
    "        return cloud_uri\n",
    "\n",
    "    def download_table(bq_table_uri: str):\n",
    "        bq_table_uri = remove_prefix(bq_table_uri,\"bq://\")\n",
    "        table = bigquery.TableReference.from_string(bq_table_uri)\n",
    "        rows = bqclient.list_rows(\n",
    "            table,\n",
    "        )\n",
    "        return rows.to_dataframe(create_bqstorage_client=False)\n",
    "    \n",
    "    print(\"generating statistics\")\n",
    "    \n",
    "    df = download_table(bq_source)\n",
    "    stats = tfdv.generate_statistics_from_dataframe(df)\n",
    "    tfdv.write_stats_text(stats,output_path)\n",
    "    \n",
    "    statistics.uri = output_path\n",
    "        \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "effd32f8-2cf1-4333-8404-7a54339e4ee7",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%writefile my-kfp-library/pipelines/bq_preprocess_train_pipeline.py\n",
    "\n",
    "import argparse\n",
    "\n",
    "def build_pipeline(args):\n",
    "    \n",
    "    import kfp\n",
    "    from kfp.v2 import compiler, dsl\n",
    "    import kfp.dsl as dsl\n",
    "    from kfp.v2.dsl import component, pipeline\n",
    "    from google.cloud import aiplatform\n",
    "    import google_cloud_pipeline_components as gcpc\n",
    "    from google_cloud_pipeline_components import aiplatform as gcc_aip\n",
    "    from google_cloud_pipeline_components.experimental import vertex_notification_email\n",
    "    from datetime import datetime\n",
    "    from components.generate_statistics_component import generate_statistics\n",
    "    from components.empty_component import empty_component\n",
    "    \n",
    "    print(\"kfp version:\",kfp.__version__)\n",
    "    print(\"gcpc version:\",gcpc.__version__)\n",
    "    \n",
    "    TIMESTAMP = datetime.now().strftime(\"%Y%m%d%H%M%S\")\n",
    "    \n",
    "    bucket = args.bucket\n",
    "    bucket_name = bucket[5:]\n",
    "    \n",
    "    job_id = f\"{args.job_id}-{TIMESTAMP}\"\n",
    "    \n",
    "    pipeline_root = f\"{bucket}/{args.pipeline_root}\"\n",
    "    \n",
    "    @pipeline(name=args.pipeline_name, pipeline_root=pipeline_root)\n",
    "    def pipeline(\n",
    "        bq_source: str = args.bq_source,\n",
    "        bucket: str = args.bucket,\n",
    "        project: str = args.project_id,\n",
    "        job_id: str = args.job_id,\n",
    "        gcp_region: str = \"us-central1\"\n",
    "    ):\n",
    "        \n",
    "        notify_email_task = vertex_notification_email.VertexNotificationEmailOp(\n",
    "                recipients=args.recipients)\n",
    "            \n",
    "        with dsl.ExitHandler(notify_email_task):\n",
    "            \n",
    "            dataset_create_op = gcc_aip.TabularDatasetCreateOp(\n",
    "                display_name=\"tabular-beans-dataset\",\n",
    "                bq_source=bq_source,\n",
    "                project=project,\n",
    "                location=gcp_region\n",
    "            )\n",
    "            \n",
    "            empty_component_op = empty_component()\n",
    "            \n",
    "            generate_statistics_op = generate_statistics(bq_source=args.bq_source, \n",
    "                                                         bucket=args.bucket, \n",
    "                                                         job_id=job_id, \n",
    "                                                         project_id=args.project_id)\n",
    "            \n",
    "            training_op = gcc_aip.CustomContainerTrainingJobRunOp(\n",
    "                display_name=\"pipeline-beans-custom-train\",\n",
    "                container_uri=args.training_container_uri,\n",
    "                project=project,\n",
    "                location=gcp_region,\n",
    "                dataset=dataset_create_op.outputs[\"dataset\"],\n",
    "                staging_bucket=bucket,\n",
    "                training_fraction_split=0.8,\n",
    "                validation_fraction_split=0.1,\n",
    "                test_fraction_split=0.1,\n",
    "                bigquery_destination=args.bq_dest,\n",
    "                model_serving_container_image_uri=\"us-docker.pkg.dev/vertex-ai/prediction/sklearn-cpu.0-24:latest\",\n",
    "                model_display_name=\"scikit-beans-model-pipeline\",\n",
    "                environment_variables={\"GCS_BUCKET\" : bucket_name},\n",
    "                machine_type=\"n1-standard-4\",\n",
    "            )\n",
    "        \n",
    "            \n",
    "    \n",
    "    compiler.Compiler().compile(pipeline_func = pipeline, package_path=\"custom_train_pipeline.json\")\n",
    "    \n",
    "    pipeline_job = aiplatform.PipelineJob(\n",
    "        display_name=\"custom-train-pipeline\",\n",
    "        template_path=\"custom_train_pipeline.json\",\n",
    "        job_id=\"custom-train-pipeline-{0}\".format(TIMESTAMP),\n",
    "        enable_caching=True\n",
    "    )\n",
    "    pipeline_job.submit()\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    parser = argparse.ArgumentParser()\n",
    "    parser.add_argument('--bucket',\n",
    "                        required=True,\n",
    "                        help='gcs bucket formatted as gs://my-bucket')\n",
    "    parser.add_argument('--pipeline-root',\n",
    "                        required=True,\n",
    "                        help='name of pipeline')\n",
    "    parser.add_argument('--pipeline-name',\n",
    "                        required=True,\n",
    "                        help=\"name of pipeline\")\n",
    "    parser.add_argument('--project-id',\n",
    "                        required=True,\n",
    "                        help=\"project id\")\n",
    "    parser.add_argument(\"--bq-source\",\n",
    "                        required=True,\n",
    "                        help=\"source table\")\n",
    "    parser.add_argument(\"--bq-dest\",\n",
    "                        required=True,\n",
    "                        help=\"destination table\")\n",
    "    parser.add_argument(\"--training-container-uri\",\n",
    "                        required=True,\n",
    "                        help=\"training container uri from gcr\")\n",
    "    parser.add_argument(\"--recipients\",nargs='+',\n",
    "                       required=True,\n",
    "                       help=\"email recipients when pipeline exists\")\n",
    "    parser.add_argument(\"--job-id\",\n",
    "                        required=True,\n",
    "                        help=\"job id of your pipeline\")\n",
    "\n",
    "    args = parser.parse_args()\n",
    "    build_pipeline(args)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9449cd6c-5be7-496b-9c86-a14f45e107d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "%env PYTHONPATH=$PYTHONPATH:/home/jupyter/jfacevedo-demos/vertexai-pipelines/kfp-w-cloud-repos/my-kfp-library/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "283f6ddc-79d4-4c14-b090-de5ca75d37c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "!python my-kfp-library/pipelines/bq_preprocess_train_pipeline.py \\\n",
    "--bucket $BUCKET_ID \\\n",
    "--pipeline-root my-pipeline-root \\\n",
    "--job-id my-beans-pipeline-1234 \\\n",
    "--project-id $PROJECT_ID \\\n",
    "--pipeline-name my-first-pipeline \\\n",
    "--bq-source bq://aju-dev-demos.beans.beans1 \\\n",
    "--bq-dest bq://$PROJECT_ID \\\n",
    "--training-container-uri $IMAGE_URI \\\n",
    "--recipients myemail@google.com "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c1ee7c0e-f228-40fa-b026-7018cb4341b4",
   "metadata": {},
   "source": [
    "## Validate outputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "93c275a8-a700-4271-b12b-f11977ec2af4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow_data_validation as tfdv\n",
    "stats_file_loc=\"gs://$BUCKET_ID/my-beans-pipeline-1234/stats.pb\"\n",
    "stats = tfdv.load_stats_text('stats.pb')\n",
    "tfdv.visualize_statistics(stats)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "da20f0b9-0158-489e-a2fd-5d1c00c9d483",
   "metadata": {},
   "source": [
    "## Push code to the cloud repo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "86f30000-8e01-43fe-999f-d710369294b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%bash\n",
    "cd my-kfp-library\n",
    "git add .\n",
    "git commit -m \"initial commit\"\n",
    "git push origin master"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7102d63a-08b2-41c1-aa87-c3f588720a1f",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%bash\n",
    "cd my-training-code\n",
    "git add .\n",
    "git commit -m \"initial commit\"\n",
    "git push origin master"
   ]
  }
 ],
 "metadata": {
  "environment": {
   "kernel": "python3",
   "name": "tf2-gpu.2-8.m93",
   "type": "gcloud",
   "uri": "gcr.io/deeplearning-platform-release/tf2-gpu.2-8:m93"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
